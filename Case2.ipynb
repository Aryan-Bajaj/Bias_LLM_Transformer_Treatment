{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring all the warnings -- True\n"
     ]
    }
   ],
   "source": [
    "# Calling the Necessary Libararies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, PowerTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Ignoring all the warnings -- True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "for feature in features:\n",
    "    df[feature] = label_encoder.fit_transform(df[feature].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_data = df[df['Gender'] == 0]\n",
    "female_data = df[df['Gender'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_male = male_data.drop(columns=[target])\n",
    "y_male = male_data[target]\n",
    "X_female = female_data.drop(columns=[target])\n",
    "y_female = female_data[target]\n",
    "\n",
    "X_train_male, X_test_male, y_train_male, y_test_male = train_test_split(X_male, y_male, test_size=0.10)\n",
    "X_train_female, X_test_female, y_train_female, y_test_female = train_test_split(X_female, y_female, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male data is not normalized.\n",
      "Female data is not normalized.\n",
      "Male data is not normalized after PowerTransformer.\n",
      "This issue can occur when there is no variation (i.e., the standard deviation is zero) in one or more of the columns.\n",
      "Female data is not normalized after PowerTransformer.\n",
      "This issue can occur when there is no variation (i.e., the standard deviation is zero) in one or more of the columns.\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset once again\n",
    "data = df\n",
    "\n",
    "# Checking the status for normalization before applying the MinMaxScaler technique\n",
    "def is_normalized(data, tolerance=1e-6):\n",
    "    return np.all(np.abs(data.mean(axis=0)) < tolerance) and np.all(np.abs(data.std(axis=0) - 1) < tolerance)\n",
    "\n",
    "is_male_data_normalized = is_normalized(male_data[features])\n",
    "is_female_data_normalized = is_normalized(female_data[features])\n",
    "\n",
    "if is_male_data_normalized:\n",
    "    print(\"Male data is already normalized.\")\n",
    "else:\n",
    "    print(\"Male data is not normalized.\")\n",
    "\n",
    "if is_female_data_normalized:\n",
    "    print(\"Female data is already normalized.\")\n",
    "else:\n",
    "    print(\"Female data is not normalized.\")\n",
    "\n",
    "# Applying Min-Max-Scaler for outlier treatment, so that further analysis can be performed smoothly\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "male_data[features] = min_max_scaler.fit_transform(male_data[features])\n",
    "female_data[features] = min_max_scaler.fit_transform(female_data[features])\n",
    "\n",
    "# Applying the PowerTransformer for final normalization with 'yeo-johnson' method, according to the google scholar, it is the best method for data normalisation\n",
    "power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "\n",
    "male_data[features] = power_transformer.fit_transform(male_data[features])\n",
    "female_data[features] = power_transformer.fit_transform(female_data[features])\n",
    "\n",
    "# Checking the final status for normalization after PowerTransformer\n",
    "is_male_data_normalized = is_normalized(male_data[features])\n",
    "is_female_data_normalized = is_normalized(female_data[features])\n",
    "\n",
    "if is_male_data_normalized:\n",
    "    print(\"Male data is normalized after PowerTransformer.\")\n",
    "else:\n",
    "    print(\"Male data is not normalized after PowerTransformer.\")\n",
    "    print('This issue can occur when there is no variation (i.e., the standard deviation is zero) in one or more of the columns.')\n",
    "\n",
    "if is_female_data_normalized:\n",
    "    print(\"Female data is normalized after PowerTransformer.\")\n",
    "else:\n",
    "    print(\"Female data is not normalized after PowerTransformer.\")\n",
    "    print('This issue can occur when there is no variation (i.e., the standard deviation is zero) in one or more of the columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male data is not normalized after Isolation Forest and PowerTransformer.\n",
      "This issue can occur when there is no variation (i.e., the standard deviation is zero) in one or more of the columns.\n",
      "Female data is not normalized after Isolation Forest and PowerTransformer.\n",
      "This issue can occur when there is no variation (i.e., the standard deviation is zero) in one or more of the columns.\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset once again\n",
    "data = data\n",
    "\n",
    "# Using the Label encode categorical features once again\n",
    "label_encoder = LabelEncoder()\n",
    "for feature in features:\n",
    "    male_data[feature] = label_encoder.fit_transform(male_data[feature])\n",
    "    female_data[feature] = label_encoder.fit_transform(female_data[feature])\n",
    "\n",
    "# Extracting the relevent features and target for male and female dset for further analysis\n",
    "X_train_male = male_data[features].values\n",
    "y_train_male = male_data[target].values\n",
    "X_train_female = female_data[features].values\n",
    "y_train_female = female_data[target].values\n",
    "\n",
    "# Creating a function to calculate FPR and FNR for further analysis\n",
    "def calculate_fpr_fnr(y_true, y_pred, group_label):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    total_negatives = tn + fp\n",
    "    total_positives = fn + tp\n",
    "\n",
    "    fpr = fp / total_negatives\n",
    "    fnr = fn / total_positives\n",
    "\n",
    "    print(f\"FPR for {group_label}: {fpr}\")\n",
    "    print(f\"FNR for {group_label}: {fnr}\")\n",
    "    return fpr, fnr\n",
    "\n",
    "# Checking the normalization status after implementing Isolation Forest\n",
    "def is_normalized(data, tolerance=1e-6):\n",
    "    return np.all(np.abs(data.mean(axis=0)) < tolerance) and np.all(np.abs(data.std(axis=0) - 1) < tolerance)\n",
    "\n",
    "if is_male_data_normalized:\n",
    "    print(\"Male data is normalized after Isolation Forest and PowerTransformer.\")\n",
    "else:\n",
    "    print(\"Male data is not normalized after Isolation Forest and PowerTransformer.\")\n",
    "    print('This issue can occur when there is no variation (i.e., the standard deviation is zero) in one or more of the columns.')\n",
    "\n",
    "if is_female_data_normalized:\n",
    "    print(\"Female data is normalized after Isolation Forest and PowerTransformer.\")\n",
    "else:\n",
    "    print(\"Female data is not normalized after Isolation Forest and PowerTransformer.\")\n",
    "    print('This issue can occur when there is no variation (i.e., the standard deviation is zero) in one or more of the columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male data is not normalized after Isolation Forest and PowerTransformer.\n",
      "This issue can occur when there is no variation (i.e., the standard deviation is zero) in one or more of the columns.\n",
      "Female data is not normalized after Isolation Forest and PowerTransformer.\n",
      "This issue can occur when there is no variation (i.e., the standard deviation is zero) in one or more of the columns.\n"
     ]
    }
   ],
   "source": [
    "# Initialising the Isolation Forest instance\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fitting the Isolation Forest to the male dset\n",
    "iso_forest.fit(X_train_male)\n",
    "\n",
    "# Predicting the outliers for male dset\n",
    "outliers_male = iso_forest.predict(X_train_male)\n",
    "\n",
    "# Filtering the outliers from the train environment\n",
    "X_train_male = X_train_male[outliers_male != -1]\n",
    "y_train_male = y_train_male[outliers_male != -1]\n",
    "\n",
    "# Fitting the Isolation Forest to the female dset\n",
    "iso_forest.fit(X_train_female)\n",
    "\n",
    "# Predicting the outliers for female dset\n",
    "outliers_female = iso_forest.predict(X_train_female)\n",
    "\n",
    "# Filtering the outliers in train environment\n",
    "X_train_female = X_train_female[outliers_female != -1]\n",
    "y_train_female = y_train_female[outliers_female != -1]\n",
    "\n",
    "# Applying the PowerTransformer for data normalization process\n",
    "power_transformer = PowerTransformer()\n",
    "X_train_male = power_transformer.fit_transform(X_train_male)\n",
    "X_train_female = power_transformer.fit_transform(X_train_female)\n",
    "\n",
    "# Checking the normalization status after implementing Isolation Forest and PowerTransformer all together\n",
    "is_male_data_normalized = is_normalized(X_train_male)\n",
    "is_female_data_normalized = is_normalized(X_train_female)\n",
    "\n",
    "if is_male_data_normalized:\n",
    "    print(\"Male data is normalized after Isolation Forest and PowerTransformer.\")\n",
    "else:\n",
    "    print(\"Male data is not normalized after Isolation Forest and PowerTransformer.\")\n",
    "    print('This issue can occur when there is no variation (i.e., the standard deviation is zero) in one or more of the columns.')\n",
    "\n",
    "if is_female_data_normalized:\n",
    "    print(\"Female data is normalized after Isolation Forest and PowerTransformer.\")\n",
    "else:\n",
    "    print(\"Female data is not normalized after Isolation Forest and PowerTransformer.\")\n",
    "    print('This issue can occur when there is no variation (i.e., the standard deviation is zero) in one or more of the columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR for Males: 0.0\n",
      "FNR for Males: 1.0\n",
      "FPR for Females: 0.0\n",
      "FNR for Females: 1.0\n",
      "FPR for Males: 0.0\n",
      "FNR for Males: 1.0\n",
      "FPR for Females: 0.013333333333333334\n",
      "FNR for Females: 0.8571428571428571\n",
      "FPR for Males: 0.12\n",
      "FNR for Males: 1.0\n",
      "FPR for Females: 0.92\n",
      "FNR for Females: 0.07142857142857142\n",
      "\n",
      "Table 1: Model Accuracies for Males and Females\n",
      "\n",
      "╒═══════════════════╤═════════╤═══════════╕\n",
      "│ Model             │   Males │   Females │\n",
      "╞═══════════════════╪═════════╪═══════════╡\n",
      "│ SVM               │   84.75 │     84.27 │\n",
      "├───────────────────┼─────────┼───────────┤\n",
      "│ Random Forest     │   84.75 │     85.39 │\n",
      "├───────────────────┼─────────┼───────────┤\n",
      "│ Gradient Boosting │   74.58 │     21.35 │\n",
      "╘═══════════════════╧═════════╧═══════════╛\n",
      "\n",
      "Table 2: False Positive Rate (FPR) and False Negative Rate (FNR) for Different Models and Groups\n",
      "\n",
      "╒═══════════════════╤═════════╤═══════╤═══════╕\n",
      "│ Model             │ Group   │   FPR │   FNR │\n",
      "╞═══════════════════╪═════════╪═══════╪═══════╡\n",
      "│ SVM               │ Males   │  0    │  1    │\n",
      "├───────────────────┼─────────┼───────┼───────┤\n",
      "│ SVM               │ Females │  0    │  1    │\n",
      "├───────────────────┼─────────┼───────┼───────┤\n",
      "│ Random Forest     │ Males   │  0    │  1    │\n",
      "├───────────────────┼─────────┼───────┼───────┤\n",
      "│ Random Forest     │ Females │  0.01 │  0.86 │\n",
      "├───────────────────┼─────────┼───────┼───────┤\n",
      "│ Gradient Boosting │ Males   │  0.12 │  1    │\n",
      "├───────────────────┼─────────┼───────┼───────┤\n",
      "│ Gradient Boosting │ Females │  0.92 │  0.07 │\n",
      "╘═══════════════════╧═════════╧═══════╧═══════╛\n",
      "\n",
      "Table 3: Disparate Mistreatment Values for Different Models\n",
      "\n",
      "╒═══════════════════╤══════════════════════════╕\n",
      "│ Model             │   Disparate Mistreatment │\n",
      "╞═══════════════════╪══════════════════════════╡\n",
      "│ SVM               │                     0    │\n",
      "├───────────────────┼──────────────────────────┤\n",
      "│ Random Forest     │                     0.16 │\n",
      "├───────────────────┼──────────────────────────┤\n",
      "│ Gradient Boosting │                     1.73 │\n",
      "╘═══════════════════╧══════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate FPR and FNR for further analysis\n",
    "def calculate_fpr_fnr(y_true, y_pred, group_label):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    total_negatives = tn + fp\n",
    "    total_positives = fn + tp\n",
    "\n",
    "    fpr = fp / total_negatives\n",
    "    fnr = fn / total_positives\n",
    "\n",
    "    print(f\"FPR for {group_label}: {fpr}\")\n",
    "    print(f\"FNR for {group_label}: {fnr}\")\n",
    "    return fpr, fnr\n",
    "\n",
    "# Loading the entire dataset once again\n",
    "data = data\n",
    "\n",
    "for feature in features:\n",
    "    male_data[feature] = label_encoder.fit_transform(male_data[feature])\n",
    "    female_data[feature] = label_encoder.fit_transform(female_data[feature])\n",
    "\n",
    "# Splitting the data into training and testing sets (90% training, 10% testing)\n",
    "X_train_male, X_test_male, y_train_male, y_test_male = train_test_split(male_data[features], male_data[target], test_size=0.10, random_state=42)\n",
    "X_train_female, X_test_female, y_train_female, y_test_female = train_test_split(female_data[features], female_data[target], test_size=0.10, random_state=42)\n",
    "\n",
    "# Initialising the Isolation Forest for outlier detection\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fitting the Isolation Forest to the male data for further analysis\n",
    "iso_forest.fit(X_train_male)\n",
    "\n",
    "# Predicting the Outliers for the male dset\n",
    "outliers_male = iso_forest.predict(X_train_male)\n",
    "\n",
    "# Filtering the outliers from the training dset\n",
    "X_train_male = X_train_male[outliers_male != -1]\n",
    "y_train_male = y_train_male[outliers_male != -1]\n",
    "\n",
    "# Fitting the Isolation Forest algorithm to the female data for further analysis\n",
    "iso_forest.fit(X_train_female)\n",
    "\n",
    "# Predicting outliers for female data for further analysis\n",
    "outliers_female = iso_forest.predict(X_train_female)\n",
    "\n",
    "# Filtering the outliers form the traiing data\n",
    "X_train_female = X_train_female[outliers_female != -1]\n",
    "y_train_female = y_train_female[outliers_female != -1]\n",
    "\n",
    "# Applying PowerTransformer to normalize the data\n",
    "power_transformer = PowerTransformer()\n",
    "X_train_male = power_transformer.fit_transform(X_train_male)\n",
    "X_train_female = power_transformer.fit_transform(X_train_female)\n",
    "\n",
    "# Model training (SVC, Random Forest Classifier, Gradient Boosting Classifier) with default parameters for Males because as mentioend in the report that default showed the best accuracy\n",
    "svm_model_male = SVC()\n",
    "rf_model_male = RandomForestClassifier()\n",
    "gb_model_male = GradientBoostingClassifier()\n",
    "\n",
    "svm_model_male.fit(X_train_male, y_train_male)\n",
    "rf_model_male.fit(X_train_male, y_train_male)\n",
    "gb_model_male.fit(X_train_male, y_train_male)\n",
    "\n",
    "# Predicting on the test set for Males for furhter analysis\n",
    "svm_male_pred = svm_model_male.predict(X_test_male)\n",
    "rf_male_pred = rf_model_male.predict(X_test_male)\n",
    "gb_male_pred = gb_model_male.predict(X_test_male)\n",
    "\n",
    "# Calculating the accuracy for Males\n",
    "svm_male_accuracy = accuracy_score(y_test_male, svm_male_pred)\n",
    "rf_male_accuracy = accuracy_score(y_test_male, rf_male_pred)\n",
    "gb_male_accuracy = accuracy_score(y_test_male, gb_male_pred)\n",
    "\n",
    "# Model training (SVC, Random Forest Classifier, Gradient Boosting Classifier) with default parameters for Females (As explained in the report default parameters were the best)\n",
    "svm_model_female = SVC()\n",
    "rf_model_female = RandomForestClassifier()\n",
    "gb_model_female = GradientBoostingClassifier()\n",
    "\n",
    "svm_model_female.fit(X_train_female, y_train_female)\n",
    "rf_model_female.fit(X_train_female, y_train_female)\n",
    "gb_model_female.fit(X_train_female, y_train_female)\n",
    "\n",
    "# Predicting on the test set for Females for further analysis\n",
    "svm_female_pred = svm_model_female.predict(X_test_female)\n",
    "rf_female_pred = rf_model_female.predict(X_test_female)\n",
    "gb_female_pred = gb_model_female.predict(X_test_female)\n",
    "\n",
    "# Calculating the accuracy for Females\n",
    "svm_female_accuracy = accuracy_score(y_test_female, svm_female_pred)\n",
    "rf_female_accuracy = accuracy_score(y_test_female, rf_female_pred)\n",
    "gb_female_accuracy = accuracy_score(y_test_female, gb_female_pred)\n",
    "\n",
    "# Formating accuracy to percentages with 2 decimal places for better representation\n",
    "svm_male_accuracy_percentage = round(svm_male_accuracy * 100, 2)\n",
    "rf_male_accuracy_percentage = round(rf_male_accuracy * 100, 2)\n",
    "gb_male_accuracy_percentage = round(gb_male_accuracy * 100, 2)\n",
    "\n",
    "svm_female_accuracy_percentage = round(svm_female_accuracy * 100, 2)\n",
    "rf_female_accuracy_percentage = round(rf_female_accuracy * 100, 2)\n",
    "gb_female_accuracy_percentage = round(gb_female_accuracy * 100, 2)\n",
    "\n",
    "# Calculating FPR (false Positive rate) and FNR (Flase negative Rate) for SVM in both groups\n",
    "fpr_male_svm, fnr_male_svm = calculate_fpr_fnr(y_test_male, svm_male_pred, \"Males\")\n",
    "fpr_female_svm, fnr_female_svm = calculate_fpr_fnr(y_test_female, svm_female_pred, \"Females\")\n",
    "\n",
    "# Calculating FPR (false Positive rate) and FNR (Flase negative Rate) for Random Forest in both groups\n",
    "fpr_male_rf, fnr_male_rf = calculate_fpr_fnr(y_test_male, rf_male_pred, \"Males\")\n",
    "fpr_female_rf, fnr_female_rf = calculate_fpr_fnr(y_test_female, rf_female_pred, \"Females\")\n",
    "\n",
    "# Calculating the FPR (false Positive rate) and FNR (Flase negative Rate) for Gradient Boosting in both groups\n",
    "fpr_male_gb, fnr_male_gb = calculate_fpr_fnr(y_test_male, gb_male_pred, \"Males\")\n",
    "fpr_female_gb, fnr_female_gb = calculate_fpr_fnr(y_test_female, gb_female_pred, \"Females\")\n",
    "\n",
    "# Table 1: Storing Model Accuracies for Males and Females in variable table4\n",
    "table4 = [[\"Model\", \"Males\", \"Females\"],\n",
    "         [\"SVM\", svm_male_accuracy_percentage, svm_female_accuracy_percentage],\n",
    "         [\"Random Forest\", rf_male_accuracy_percentage, rf_female_accuracy_percentage],\n",
    "         [\"Gradient Boosting\", gb_male_accuracy_percentage, gb_female_accuracy_percentage]]\n",
    "\n",
    "# Printing the 1st table\n",
    "print('')\n",
    "print(\"Table 1: Model Accuracies for Males and Females\")\n",
    "print('')\n",
    "print(tabulate(table4, headers=\"firstrow\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "# Table 2: Storing False Positive Rate (FPR) and False Negative Rate (FNR) for Different Models and Groups all together\n",
    "table5 = [\n",
    "    [\"Model\", \"Group\", \"FPR\", \"FNR\"],\n",
    "    [\"SVM\", \"Males\", round(fpr_male_svm, 2), round(fnr_male_svm, 2)],\n",
    "    [\"SVM\", \"Females\", round(fpr_female_svm, 2), round(fnr_female_svm, 2)],\n",
    "    [\"Random Forest\", \"Males\", round(fpr_male_rf, 2), round(fnr_male_rf, 2)],\n",
    "    [\"Random Forest\", \"Females\", round(fpr_female_rf, 2), round(fnr_female_rf, 2)],\n",
    "    [\"Gradient Boosting\", \"Males\", round(fpr_male_gb, 2), round(fnr_male_gb, 2)],\n",
    "    [\"Gradient Boosting\", \"Females\", round(fpr_female_gb, 2), round(fnr_female_gb, 2)],\n",
    "]\n",
    "\n",
    "# Printing the 2nd table\n",
    "print('')\n",
    "print(\"Table 2: False Positive Rate (FPR) and False Negative Rate (FNR) for Different Models and Groups\")\n",
    "print('')\n",
    "print(tabulate(table5, headers=\"firstrow\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "# Calculating the Disparate Mistreatment in both groups for each of the algorithms\n",
    "disparate_mistreatment_svm = {\n",
    "    \"SVM\": abs(fpr_male_svm - fpr_female_svm) + abs(fnr_male_svm - fnr_female_svm)\n",
    "}\n",
    "\n",
    "disparate_mistreatment_rf = {\n",
    "    \"Random Forest\": abs(fpr_male_rf - fpr_female_rf) + abs(fnr_male_rf - fnr_female_rf)\n",
    "}\n",
    "\n",
    "disparate_mistreatment_gb = {\n",
    "    \"Gradient Boosting\": abs(fpr_male_gb - fpr_female_gb) + abs(fnr_male_gb - fnr_female_gb)\n",
    "}\n",
    "\n",
    "# Table 3: Displaying the Disparate Mistreatment Values for Different Models\n",
    "table6 = []\n",
    "table6.append([\"Model\", \"Disparate Mistreatment\"])\n",
    "\n",
    "# Adding the values to the SVM/SVC\n",
    "table6.append([\"SVM\", round(disparate_mistreatment_svm['SVM'], 2)])\n",
    "\n",
    "# Adding the values to Random Forest Classifier Algorithm\n",
    "table6.append([\"Random Forest\", round(disparate_mistreatment_rf['Random Forest'], 2)])\n",
    "\n",
    "# Adding the values to the Gradient Boosting Classifier algorith\n",
    "table6.append([\"Gradient Boosting\", round(disparate_mistreatment_gb['Gradient Boosting'], 2)])\n",
    "\n",
    "# Printing the 3rd table\n",
    "print('')\n",
    "print(\"Table 3: Disparate Mistreatment Values for Different Models\")\n",
    "print('')\n",
    "print(tabulate(table6, headers=\"firstrow\", tablefmt=\"fancy_grid\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2022.10.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
